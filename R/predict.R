#' Make predictions
#'
#' @param models model_list object, as from `tune_models`
#' @param newdata data frame with same structure as the input to `train_models`
#'   or `tune_models` that generated `models`. If the data that models were
#'   tuned on was prepaired via `prep_data`, newdata will be prepared the same
#'   way, unless prepdata is FALSE.
#' @param prepdata Logical, rarely needs to be set by the user. By default, if
#'   `newdata` hasn't been prepped, it will be prepped by `prep_data` before
#'   predictions are made. Set this to TRUE to force already-prepped data
#'   through `prep_data` again, or set to FALSE to prevent `newdata` from being
#'   sent through `prep_data`.
#' @return A tibble data frame: newdata with an additional column for the
#'   predictions in "predicted_TARGET" where TARGET is the name of the variable
#'   being predicted. If classification, the new column will contain predicted
#'   probabilities. The tibble will have child class "hcai_predicted_df" and
#'   attribute "model_info" that contains information about the model used to
#'   make predictions.
#' @export
#' @importFrom caret predict.train
#'
#' @details prepping data inside `predict` has the advantage of returning your
#'   predictions with the data frame in its original (unprepped) format. To do
#'   this, simply pass the data frame on which predictions are to be generated
#'   to `newdata` in the same format as the training data was passed to
#'   `prep_data` or `train_models`.
#'
#' @examples
#' library(dplyr)
#' # Make a small, 50 row data frame with no missingness
#' small_data <-
#'   pima_diabetes %>%
#'   stats::na.omit() %>%
#'   sample_n(50)
#' # Split the data into 80% for model training and 20% for testing
#' train_rows <- caret::createDataPartition(small_data$skinfold, p = .8)[[1]]
#' training <- slice(small_data, train_rows)
#' testing <- slice(small_data, -train_rows)
#' # Prepare the training data by centering and scaling numeric variables
#' # and one-hot encoding categorical variables
#' prepped_training <- prep_data(training, skinfold, patient_id,
#'                               center = TRUE, scale = TRUE, dummies = TRUE)
#' Tune models. Because patient_id was ignored it has to be removed
#' models <- tune_models(select(prepped_training, -patient_id), skinfold)
#' predictions <- predict(models, testing)
#' predictions$predicted_skinfold
predict.model_list <- function(models, newdata, prepdata) {
  if (!inherits(newdata, "data.frame"))
    stop("newdata must be a data frame")
  mi <- extract_model_info(models)
  best_models <- models[[mi$best_model_name]]
  # If prepdata provided by user; follow that. Else, prep if newdata hasn't been
  prep <-
    if (!missing(prepdata)) {
      prepdata
    } else {
      # Compare names and classes in training and newdata, except target...
      same_vars <-
        all.equal(get_classes_sorted(dplyr::select(newdata, -which(names(newdata) == mi$target))), # nolint
                  get_classes_sorted(dplyr::select(best_models$trainingData, -.outcome))) # nolint
      # ...If all are the same or newdata has class hcai_prepped_df, don't prep
      !(isTRUE(same_vars) || inherits(newdata, "hcai_prepped_df"))
    }
  # If classification, want probabilities. If regression, raw's the only option
  type <- if (is.classification_list(models)) "prob" else "raw"
  # This bit of repition avoids copying newdata if it's not being prepped
  preds <-
    if (prep) {
      prep_data(newdata, rec_obj = attr(models, "rec_obj")) %>%
        caret::predict.train(best_models, ., type = type)
    } else {
      newdata %>%
        caret::predict.train(best_models, ., type = type)
    }
  if (is.data.frame(preds)) preds <- dplyr::pull(preds, Y)
  newdata[[paste0("predicted_", mi$target)]] <- preds
  newdata <- tibble::as_tibble(newdata)
  class(newdata) <- c("hcai_predicted_df", class(newdata))
  attr(newdata, "model_info") <-
    list(target = mi$target,
         algorithm = mi$best_model_name,
         metric = mi$metric,
         performance = mi$best_model_perf,
         hyperparameters = structure(mi$best_model_tune,
                                     "row.names" = "optimal:"))
  return(newdata)
}

#' Print method for predicted data frame
#' @export
#' @param d data frame from `predict.model_list`
#' @return d
#' @noRd
print.hcai_predicted_df <- function(d, ...) {
  mi <- attr(d, "model_info")
  mes <- paste0("predicted_", mi$target, " generated by ", mi$algorithm
                # , ". Training performance (out-of-fold) was ", mi$metric,
                # " = ", round(mi$performance, 3)
                )
  message(mes)
  tibble:::print.tbl_df(d)
  return(invisible(d))
}

#' get_classes_sorted
#' @noRd
get_classes_sorted <- function(d) {
  classes <- purrr::map_chr(d, class)
  return(classes[order(names(classes))])
}
