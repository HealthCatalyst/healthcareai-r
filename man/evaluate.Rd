% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluate.R
\name{evaluate}
\alias{evaluate}
\alias{evaluate.hcai_predicted_df}
\alias{evaluate.model_list}
\title{Get model performance metrics}
\usage{
evaluate(x, ...)

\method{evaluate}{hcai_predicted_df}(x, ...)

\method{evaluate}{model_list}(x, ...)
}
\arguments{
\item{x}{Object that should be evalutes}

\item{...}{Other arguments passed to specific methods}
}
\description{
Get model performance metrics
}
\details{
This function is a generic that can be used to get model performance
  on a model_list object that comes from \code{\link{machine_learn}},
  \code{\link{tune_models}}, \code{\link{flash_models}}, or a data frame of
  predictions from \code{\link{predict.model_list}}. For the latter, the data
  passed to \code{predict.model_list} must contain observed outcomes. If you
  have predictions and outcomes in a different format, see
  \code{\link{evaluate_classification}} or \code{\link{evaluate_regression}}
  instead.
}
\examples{
models <- machine_learn(pima_diabetes[1:40, ], patient_id, outcome = diabetes,
                        models = "rf", tune_depth = 3)
# evaluate(models)
predictions <- predict(models, newdata = pima_diabetes[41:50, ])
evaluate(predictions)
}
